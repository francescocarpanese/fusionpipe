{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"best_practices_jupyter_lab/","title":"Develop with Jupyter Lab","text":"<p>TODO</p>"},{"location":"best_practices_jupyter_vscode/","title":"Develop with Jupyter Notebooks in VSCode","text":"<p>TODO</p>"},{"location":"best_practices_pipeline_package/","title":"Best Practices: Developing a Reusable Pipeline Package","text":"<p>When building a new pipeline, you'll often want to develop an associated Python package. This allows you to version control your code, share it between pipeline nodes, and reuse it in future projects. This guide outlines the recommended strategy for creating and integrating a reusable package into your pipeline workflow.</p>"},{"location":"best_practices_pipeline_package/#part-1-creating-your-python-package","title":"Part 1: Creating Your Python Package","text":"<p>First, set up a new Python package. We recommend using <code>uv</code> for initialization.</p>"},{"location":"best_practices_pipeline_package/#1-initialize-the-package","title":"1. Initialize the Package","text":"<p>Run the following command to create a new package: <pre><code>uv init --package &lt;mypackage&gt;\n</code></pre></p> <p>This command scaffolds a standard Python project structure for you:</p> <pre><code>&lt;mypackage&gt;/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 &lt;mypackage&gt;/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 your_function.py\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <ul> <li><code>pyproject.toml</code>: A configuration file where you can list your project's dependencies.</li> <li><code>src/&lt;mypackage&gt;</code>: The directory where your package's source code lives.</li> </ul>"},{"location":"best_practices_pipeline_package/#2-set-up-version-control-with-git","title":"2. Set Up Version Control with Git","text":"<p>The <code>uv init</code> command also initializes a Git repository. To publish your package, you'll need to create a remote repository on a platform like GitHub or GitLab.</p> <ol> <li> <p>Create an empty repository on GitHub or GitLab. Name it <code>&lt;mypackage&gt;</code>.</p> </li> <li> <p>Navigate into your package directory: <pre><code>cd &lt;mypackage&gt;\n</code></pre></p> </li> <li> <p>Link your local repository to the remote one: <pre><code>git remote add origin git@github.com:&lt;your-username&gt;/&lt;mypackage&gt;.git\n</code></pre></p> </li> <li> <p>Push your initial code to the remote repository: <pre><code>git push -u origin main\n</code></pre></p> </li> </ol> <p>Your package is now set up for version-controlled development.</p> <p>Switching Default Branch from <code>master</code> to <code>main</code></p> <p>If your remote repository uses <code>master</code> as the default branch, you can rename it to <code>main</code> for consistency. Run the following commands:</p> <pre><code>git branch -m master main\ngit push -u origin main\ngit symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main\n</code></pre> <p>Then, update the default branch in your repository settings on GitHub or GitLab to <code>main</code>, and delete the old <code>master</code> branch if desired:</p> <pre><code>git push origin --delete master\n</code></pre>"},{"location":"best_practices_pipeline_package/#part-2-integrating-the-package-into-a-pipeline-node","title":"Part 2: Integrating the Package into a Pipeline Node","text":"<p>With your package created, you can now integrate it into a pipeline node.</p>"},{"location":"best_practices_pipeline_package/#1-prepare-the-node","title":"1. Prepare the Node","text":"<p>First, create a new pipeline and a node within it. Then, navigate to the node's code directory:</p> <pre><code>cd &lt;node_id&gt;/code\n</code></pre>"},{"location":"best_practices_pipeline_package/#2-clone-your-package","title":"2. Clone Your Package","text":"<p>Clone your newly created package into this directory:</p> <pre><code>git clone &lt;package_url&gt;\n</code></pre> <p>The directory structure inside your node's <code>code</code> folder should now look like this:</p> <pre><code>&lt;node_id&gt;/\n\u251c\u2500\u2500 code/\n\u2502   \u251c\u2500\u2500 &lt;mypackage&gt;/      # Your cloned package\n\u2502   \u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502   \u251c\u2500\u2500 main.py         # The node's main script\n\u2502   \u2514\u2500\u2500 pyproject.toml    # The node's environment\n\u251c\u2500\u2500 data/\n\u2514\u2500\u2500 reports/\n</code></pre>"},{"location":"best_practices_pipeline_package/#3-install-the-package","title":"3. Install the Package","text":"<p>To make your package's functions available to the node's <code>main.py</code> script, install it in editable mode using <code>uv</code>:</p> <pre><code>uv pip install --editable &lt;mypackage&gt;\n</code></pre> <p>What is Editable Mode?</p> <p>The <code>--editable</code> flag creates a symbolic link from your node's Python environment to your package's source code. This means any changes you make to the package's code are immediately available to the node without needing to reinstall it.</p>"},{"location":"best_practices_pipeline_package/#part-3-developing-and-using-your-code","title":"Part 3: Developing and Using Your Code","text":"<p>Now you can develop your package's functionality and use it within the pipeline.</p>"},{"location":"best_practices_pipeline_package/#1-add-code-to-your-package","title":"1. Add Code to Your Package","text":"<p>Write reusable functions inside your package's source directory (<code>&lt;mypackage&gt;/src/&lt;mypackage&gt;</code>). For example, create a file named <code>dataset.py</code> with a function to process data from parent nodes:</p> <pre><code># In &lt;mypackage&gt;/src/&lt;mypackage&gt;/dataset.py\ndef print_parents(parent_folders):\n    for folder in parent_folders:\n        print(f\"Found parent folder: {folder}\")\n</code></pre>"},{"location":"best_practices_pipeline_package/#2-use-your-package-in-mainpy","title":"2. Use Your Package in <code>main.py</code>","text":"<p>Import and call your package's functions from the node's <code>main.py</code> script.</p> <pre><code># In &lt;node_id&gt;/code/main.py\n\n# Import the user API to interact with the pipeline\nfrom python_user_utils.node_api import get_all_parent_node_folder_paths, get_node_id\n\n# Import the function from your package\nfrom mypackage.dataset import print_parents\n\nif __name__ == \"__main__\":\n    node_id = get_node_id()\n\n    # Get the paths of all parent nodes\n    parent_folders = get_all_parent_node_folder_paths(node_id=node_id)\n\n    # Use the function from your package to process them\n    print_parents(parent_folders)\n</code></pre>"},{"location":"best_practices_pipeline_package/#3-commit-and-push-your-changes","title":"3. Commit and Push Your Changes","text":"<p>After developing a new feature, commit and push the changes to your package's repository:</p> <pre><code># Navigate to your package directory\ncd &lt;mypackage&gt;\n\n# Stage, commit, and push your changes\ngit add src/\ngit commit -m \"feat: Add print_parents function\"\ngit push\n</code></pre>"},{"location":"best_practices_pipeline_package/#part-4-testing-your-node","title":"Part 4: Testing Your Node","text":"<p>Before running the full pipeline, test your node's script in isolation.</p> <ol> <li> <p>Navigate to the node's code directory: <pre><code>cd &lt;node_id&gt;/code\n</code></pre></p> </li> <li> <p>Run the main script: <pre><code>uv run python main.py\n</code></pre></p> </li> </ol> <p>Implement a <code>preview</code> Mode for Faster Development</p> <p>When working with large datasets, consider adding a <code>preview</code> flag or option to your functions. This allows you to run them on a small subset of data for quick testing and iteration, saving you time.</p> <p>Once you confirm the script runs correctly, you can execute the entire pipeline from the UI. For more details, see the Develop a Node guide.</p>"},{"location":"best_practices_pipeline_package/#part-5-reusing-your-package-in-other-nodes","title":"Part 5: Reusing Your Package in Other Nodes","text":"<p>A key benefit of this approach is reproducibility. Each node has a distinct snapshot of your package. When creating new nodes that depend on your package, you have two main options:</p>"},{"location":"best_practices_pipeline_package/#a-duplicate-an-existing-node","title":"a) Duplicate an Existing Node","text":"<p>Duplicate a node that already contains your package. This is the simplest method. After duplicating, navigate to the package directory and pull the latest changes to ensure it's up to date.</p> <pre><code>cd &lt;new_node_id&gt;/code/&lt;mypackage&gt;\ngit pull\n</code></pre>"},{"location":"best_practices_pipeline_package/#b-clone-the-package-into-a-new-node","title":"b) Clone the Package into a New Node","text":"<p>Alternatively, clone the package into the new node's <code>code</code> directory and install it in editable mode, following the same steps outlined in Part 2.</p> <pre><code>cd &lt;new_node_id&gt;/code\ngit clone &lt;package_url&gt;\nuv pip install --editable &lt;mypackage&gt;\n</code></pre> <p>By keeping a local copy of the package in each node, you ensure that your pipeline remains reproducible, even as your package evolves over time. For a deeper comparison of different dependency management strategies, see External Python Dependencies.</p>"},{"location":"best_practices_vscode/","title":"Develop with VSCode","text":"<p>TODO</p>"},{"location":"dev_maintainer_guide/","title":"Multiple Users (Bare metal)","text":"<p>This is the developer/maintainer guide for <code>fusionpipe</code>.</p> <p><code>fusionpipe</code> is lightwise pipeline orchestrator to help organise data-analysis, simulation, machine learning pipelines. It is composed by the following components:</p> <ul> <li>Frontend in Svelte. </li> <li>FASTAPI web-app as backend.</li> <li>A postgresSQL database which is used </li> </ul> <p>With the current guidelines <code>fusionpipe</code>  This guide has different level of complexity depending on what is your role an your deployment solution:</p> <ul> <li>user (without installation): In case you are only using <code>fusionpipe</code> which is already set-up by your IT, refer to the user guide</li> <li>user (with installation): In case you would like to run <code>fusionpipe</code> locally on your device and use it. This is the same set-up as for the maintainer. If you are the only user, the installation is simplified.</li> <li>developer: In this case you would like to contribute to the development of <code>fusionpipe</code>. You will have some special command to run the different systems in debugging mode. It is expected that in this set-up you will be developing the application as a single user.</li> <li>maintainer: If you are a maintainer of <code>fusionpipe</code>, you are deploying fusion for yourself and potentially other users. In case you will have multiple users using the service, you will need some extra set-up to grant the necessary permission to the different users. </li> </ul> <p>Some operation during the installation process will require to have sudo access. Reach out to your admin if needed. The operation that requires sudo are:</p> <ul> <li>Setting up <code>postgresSQL</code> </li> <li>Grant access to <code>docker</code> group to your user.</li> <li>Extend permission r/w permission if setting up the deployment for multiple users.</li> <li>Networking in case you are deploying the solution on a server which firewall and port restriction.</li> </ul> <p>In summary <code>fusionpipe</code> has the following things to be set-up:</p> <ul> <li>Svelte application</li> <li>FAST-API backend</li> <li>PostgresSQL database</li> </ul> <p>In case of multiple users installation, also the following needs to be set-up.</p> <ul> <li>Shared access folder for the data.</li> <li>Shared access folder for the <code>uv</code> caching folder environment.</li> <li>Shared access for the user API utilities folder.</li> </ul>"},{"location":"dev_maintainer_guide/#prerequistes","title":"Prerequistes","text":""},{"location":"dev_maintainer_guide/#postgres","title":"Postgres","text":"<p>PostgreSQL is used as a database to keep track of the relation between nodes and pipeline and other metadata. - Set-up postgres in your local machine. - Use docker to set up postgres.</p>"},{"location":"dev_maintainer_guide/#local-installation-need-sudo","title":"local installation (need sudo)","text":"<ul> <li> <p>Install postgres</p> </li> <li> <p>Initialise postgtes <code>sudo /usr/bin/postgresql-setup --initdb</code></p> </li> </ul> <p>Usually postgres is initialised creating the OS user <code>postgres</code>, which serves as admin the database operation.</p> <p>The default set-up for postgres to check the OS user, to grant access to the database (<code>peer</code> option).  In this condition, in order to perform admin operation on the database, you need to log as <code>postgres</code> user (requires sudo access).</p> <p>It is possibile to change the postgres authentication defualt, changing the file <code>pg_hba.conf</code> usually found in the location <code>/var/lib/pgsql/data/pg_hba.conf</code>. Changing <code>peer</code> to <code>md5</code> will ask for password authentication at the login instead of </p> <p>After changing the authorisatio, reload postgres <pre><code>sudo systemctl reload postgresql\n</code></pre></p>"},{"location":"dev_maintainer_guide/#docker-installation","title":"docker installation","text":"<ul> <li>TO BE UPDATED *</li> </ul> <p>Set the env variable.</p> <ul> <li> <p>Modify the variables into the <code>developer.env</code> file.</p> </li> <li> <p>Navigate the docker folder <pre><code>cd docker\n</code></pre></p> </li> <li> <p>Set up the variables in <code>docker-compose-psg.yml</code>. See documentation in the file.</p> </li> <li>If you already have a database and you want to remove all the data related to it, delete the folder with the database location. Otherwise, to continue with the existing database, skip this step.</li> <li>Run the docker compose command to start the postgres database. <pre><code>docker-compose -f docker-compose-psg.yml up\n</code></pre> This will startup yuor postgres database in a docker container, and expose to the port specified in the <code>docker-compose-psg.yml</code> file.</li> </ul> <p>Test connection <pre><code>psql -h localhost -p 5542 -U &lt;user&gt; -d &lt;database&gt;\n</code></pre></p>"},{"location":"dev_maintainer_guide/#uv-package-manager","title":"UV package manager","text":"<p>UV package manager is the solution used to in order to deal with python packages.</p>"},{"location":"dev_maintainer_guide/#set-up-the-database","title":"Set-up the database","text":"<p>The following commands needs to be run from the OS user <code>postgres</code> which has all the access. It is assumed that there is a priviledge OS user, which is the maintainer/developer for <code>fusionpipe</code> in your machine. There are different possibilities for that.</p> <ul> <li>If you are the only user/developer, use your user as admin.</li> <li>If you are the maintainer for fusionpipe on a server, we recommend to create an OS user which will manage the <code>fusionpipe</code> installation. We will assume this user to be to be called <code>fusionpipeadmin</code> in the following.</li> </ul> <p>The following opeartion must be done in the order they are presented</p> <ul> <li> <p>Log in as <code>postgres</code> user in your machine <pre><code>sudo -u postgres -i\n</code></pre></p> </li> <li> <p>Create the database <pre><code>createdb fusionpipe_prod1\n</code></pre></p> </li> <li> <p>Check that the database has been created <pre><code>psql -U postgres -l\n</code></pre></p> </li> <li> <p>Create a ROLE which will be able to modify the database. User will be added to this role <pre><code>psql -c \"CREATE ROLE fusionpipeusers NOLOGIN;\"\n</code></pre></p> </li> <li> <p>Grant connection to the role <code>fusionpipeusers</code> <pre><code>psql -U postgres -d fusionpipe_prod1 -c \"GRANT CONNECT ON DATABASE fusionpipe_prod1 TO fusionpipeusers;\"\n</code></pre></p> </li> <li> <p>Grant usage to the role <code>fusionpipeusers</code> <pre><code>psql -d fusionpipe_prod1 -c \"GRANT USAGE ON SCHEMA public TO fusionpipeusers;\"\n</code></pre></p> </li> <li> <p>As we want only the admin user to have the possibility to create table in the datanase, revoke some access access to other users. <pre><code>psql -d fusionpipe_prod1 -c \"REVOKE CREATE ON SCHEMA public FROM PUBLIC;\"\n</code></pre> <pre><code>psql -d fusionpipe_prod1 -c \"REVOKE CREATE ON SCHEMA public FROM fusionpipeusers;\"\n</code></pre></p> </li> <li> <p>Grant the possibility to the ROLE <code>fusionusers</code> to be modify the table in the database. <pre><code>psql -d fusionpipe_prod1 -c \"GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO fusionpipeusers;\"\n</code></pre></p> </li> <li> <p>Grant the possibiliy to possibility to create tables to the <code>fusionpipeadmin</code>. <pre><code>psql -d fusionpipe_prod1 -c \"GRANT CREATE ON SCHEMA public TO fusionpipeadmin;\"\n</code></pre></p> </li> <li> <p>Extend the priviledges of <code>fusionpipeusers</code> to <code>fusionpipeadmin</code>. When you add <code>fusionpipeadmin</code> to the <code>fusionpipeusers</code> role in PostgreSQL:</p> </li> </ul> <pre><code>psql -U postgres -d fusionpipe_prod1 -c \"GRANT fusionpipeusers TO fusionpipeadmin;\"\n</code></pre> <p>This extends the permissions of <code>fusionpipeadmin</code> to include all privileges granted to <code>fusionpipeusers</code>. The <code>fusionpipeadmin</code> will retain its own privileges and additionally inherit those of <code>fusionpipeusers</code>. It does not restrict <code>fusionpipeadmin</code> to only the permissions of <code>fusionpipeusers</code>; instead, it is a superset of both.</p> <ul> <li>The following command updates the default privileges for tables created in the <code>public</code> schema of the <code>fusionpipe_prod1</code> database. It ensures that any new tables will automatically grant <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> permissions to the <code>fusionpipeusers</code> role. Run this command as the <code>fusionpipeadmin</code> user: <pre><code>psql -U fusionpipeadmin -d fusionpipe_prod1 -c \"ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO fusionpipeusers;\"\n</code></pre></li> </ul>"},{"location":"dev_maintainer_guide/#create-and-initialise-the-required-tables-for-database","title":"Create and initialise the required tables for database","text":"<p>\u26a0\ufe0f WARNING: The following operations will modify your PostgreSQL database. Ensure you have backed up any important data before proceeding. Only users with the appropriate privileges should perform these actions.</p> <p>The database needs to be initialised with the tables for the <code>fusionpipe</code> application. Given the role that have been granted in the previous step, only the <code>fusionpipeadmin</code> user is able to create this.</p> <ul> <li> <p>Add to the <code>.bash_profile</code> of <code>fusionpipeadmin</code> user. This will allow the <code>fusionpipeuser</code> to connect to the database through <code>peer</code> connection. (See postgres SQL documentation). <pre><code>export DATABASE_URL=\"dbname=fusionpipe_prod1 port=5432\"\n</code></pre></p> </li> <li> <p>Initialise the tables in the database. This is conveniently done runnig the script.  <pre><code>uv run python fusionpipe/src/fusionpipe/utils/init_database.py\n</code></pre></p> </li> </ul>"},{"location":"dev_maintainer_guide/#create-data-folder-and-extend-rw-access-for-multiple-users","title":"Create data folder and extend R/W access for multiple users","text":"<ul> <li>Create the folder that will contain application data for <code>fusionpipe</code> data application.  <pre><code>mkdir &lt;fusion_pipe_data_folder&gt;\n</code></pre></li> </ul> <p>The following steps are only needed if you are setting-up <code>fusionpipe</code> for multiple users to have access to your machine. If you are single developer, or single user you can skip them.</p> <ul> <li> <p>Create a group. This way you will be able to control the user access to data based on the group. <pre><code>sudo groupadd fusionpipeusers\n</code></pre></p> </li> <li> <p>Log out and log in again to allow the propagation of the new group.</p> </li> <li> <p>Add <code>fusionpipeadmin</code> admin user to the group <pre><code>sudo usermod -aG fusionpipeusers fusionpipeadmin\n</code></pre></p> </li> <li> <p>Log out and log in every time you add a new user.</p> </li> <li> <p>Set group permission permission to the shared data folder <pre><code>sudo chown -R :fusionpipeusers &lt;fusion_pipe_data_folder&gt;\nsudo chmod -R 2770 &lt;fusion_pipe_data_folder&gt;\n</code></pre></p> </li> <li> <p>Enforce Permissions with ACLs. This will let other user to write on files generated by other users, independently of the user mask. <pre><code>sudo setfacl -d -m g::rwx /misc/fusionpipe_shared\nsudo setfacl -d -m o::--- /misc/fusionpipe_shared\n</code></pre> The second special folder is the one containing the user utility scripts. This needs to be (only) readable by all users of <code>fusionpipe</code>.</p> </li> <li> <p>Login with <code>fusionpipeadmin</code> in the folder containing the source code of fusion pipe.</p> </li> <li>Grant reading access to the group <code>fusionpipeusers</code> <pre><code>sudo chown -R :fusionpipeusers fusionpipe/src/fusionpipe/user_utils\nsudo chmod -R 2750 fusionpipe/src/fusionpipe/user_utils\n</code></pre></li> </ul>"},{"location":"dev_maintainer_guide/#set-enviroment-variables","title":"Set enviroment variables","text":"<p>If you are the maintainer or single developer user, we recommend to collect all your environment variables into a single file <code>&lt;myenvfile&gt;.env</code>. Careful with the syntax, <code>.env</code> file does not allow for spaces in the definition definition of the environment variables. The strings needs to be withing <code>\"\"</code> and not <code>''</code>.  The full set of environment variables needed to run all services of <code>fusionpipe</code> are:</p> <pre><code>FUSIONPIPE_DATA_PATH=\"&lt;absolute/path/to/fusionpipe/data/folder&gt;\"\nUV_CACHE_DIR=\"&lt;absolute/path/to/uv/cache/dir&gt;\"\nDATABASE_URL=\"dbname=&lt;database_name&gt; port=&lt;postgres_port&gt;\"\nBACKEND_HOST=\"localhost\"\nBACKEND_PORT=&lt;backend_port&gt;\nVITE_BACKEND_PORT=${BACKEND_PORT}\nVITE_BACKEND_HOST=${BACKEND_HOST}\nVITE_FRONTEND_PORT=&lt;frontend_port&gt;\nVITE_FRONTEND_HOST=\"localhost\"\nUSER_UTILS_FOLDER_PATH=\"&lt;absolute/path/to/user/utils&gt;\"\nFP_MATLAB_RUNNER_PATH=\"&lt;abosolute/path/to/matlab/executable&gt;\"\nVIRTUAL_ENV=\"&lt;(optional) default virtual env to activate when running the backend&gt;\"\nDATABASE_URL_TEST=\"dbname=&lt;database_test_name&gt; port=&lt;postgres_port&gt;\"\n</code></pre> <p>In the following some explanation of the different environment variables: </p> <ul> <li><code>&lt;absolute/path/to/fusionpipe/data/folder&gt;</code>: Data from <code>fusionpipe</code> will be stored in this folder.</li> <li><code>&lt;absolute/path/to/uv/cache/dir&gt;</code>: This will contain the cache for the uv environment.</li> <li><code>&lt;database_name&gt;</code>: The database name that you have created in previous steps.</li> <li><code>&lt;postgres_port&gt;</code>: The port where postgresSQL is avilable in your host. Defaulat 5432</li> <li><code>&lt;backend_port&gt;</code>: Port for the FAST-API backend. We recommend to user a port &gt;8000</li> <li><code>&lt;frontend_port&gt;</code>: This is the port for Svelte frontent. Usually Svelte is using a port &gt;5000</li> <li><code>&lt;absolute/path/to/user/utils&gt;</code>: This is the absolute path where the user utilities, which needs to be readable by all users, are stored.</li> <li><code>&lt;abosolute/path/to/matlab/executable&gt;</code>: It is conveninet to set-up the path to your local installation of matlab if you are considering using it for development.</li> <li><code>&lt;database_test_name&gt;</code>: The database name used for the test suite. Usually <code>fusionpipe_test</code>.</li> </ul>"},{"location":"dev_maintainer_guide/#run-the-frontend-and-backend-developer-mode","title":"Run the frontend and backend (developer mode)","text":"<ul> <li> <p>Open a new terminal</p> </li> <li> <p>Set the environment variable for the terminal. You can set all the variables from your env file with the command. <pre><code>set -a\nsource &lt;myenvfile.env&gt;\nset +a\n</code></pre></p> </li> <li> <p>Navigate the backend folder <pre><code>cd fusionpipe/src/fusionpipe/api\n</code></pre></p> </li> <li> <p>Start the backend service <pre><code>uv run python main.py\n</code></pre></p> </li> <li> <p>Open a new terminal </p> </li> <li> <p>Set the environment variable as above also in this terminal</p> </li> <li> <p>Navigate the frontend folder <pre><code>cd fusionpipe/src/fusionpipe/frontend\n</code></pre></p> </li> <li> <p>Start the Svelte application in debug mode <pre><code>VITE_BACKEND_HOST=$VITE_BACKEND_HOST VITE_BACKEND_PORT=$VITE_BACKEND_PORT  npm run dev -- --port $VITE_FRONTEND_PORT\n</code></pre></p> </li> <li> <p>Open the browser at the port where the frontend is served. For example <pre><code>localhost:5174\n</code></pre></p> </li> </ul> <p>If your are developing the application and your environment variables are not change, you can consider to add them in your <code>.bashrc</code> or <code>.bashrc_profile</code> in order to have them loaded directly when you log with your user.</p>"},{"location":"dev_maintainer_guide/#compile-the-frontend-and-serve","title":"Compile the frontend and serve","text":"<p>If you want to compile the frontend </p> <p>You need to set the environment variable before building the app as the environemtn variable will be backed in the app <pre><code>npm run build\n</code></pre></p> <pre><code>npm run preview -- --port $VITE_FRONTEND_PORT\n</code></pre> <pre><code>npx serve -s dist -l $VITE_FRONTEND_PORT\n</code></pre>"},{"location":"dev_maintainer_guide/#run-the-frontend-and-backend-production","title":"Run the frontend and backend (production)","text":""},{"location":"dev_maintainer_guide/#run-fusionpipe-in-as-systemd-need-review-not-fully-working-at-the-moment","title":"Run fusionpipe in as systemd (Need review not fully working at the moment)","text":"<ul> <li> <p>Save the enviroment variable file in location, hidden from the user and protect it.</p> </li> <li> <p>Create the user systemd service directory (if it doesn\u2019t exist): <pre><code>mkdir -p ~/.config/systemd/user\n</code></pre></p> </li> <li> <p>Write <code>systemd</code> configuration file <code>~/.config/systemd/user/fusionpipe_backend.service</code> <pre><code>[Unit]\nDescription=Run the backend\n\n[Service]\nType=simple\nWorkingDirectory=/home/fusionpipeadmin/Documents/fusionpipe/src/fusionpipe/api\nEnvironmentFile=/home/fusionpipeadmin/Documents/environment/production.env\nExecStart=/home/fusionpipeadmin/Documents/fusionpipe/.venv/bin/uv run python main.py\nRestart=on-failure\n\n[Install]\nWantedBy=default.target\n</code></pre></p> </li> <li> <p>Write <code>systemd</code> configuration file <code>~/.config/systemd/user/fusionpipe_frontend.service</code> <pre><code>[Unit]\nDescription=Run fusionpipe frontend\n\n[Service]\nType=simple\nEnvironmentFile=/home/fusionpipeadmin/Documents/environment/production.env\nWorkingDirectory=/home/fusionpipeadmin/Documents/fusionpipe/src/fusionpipe/frontend\nExecStart=npx serve -s dist -l $VITE_FRONTEND_PORT\nRestart=on-failure\n\n[Install]\nWantedBy=default.target\n</code></pre></p> </li> <li> <p>Reload the deamon <pre><code>systemctl --user daemon-reload\n</code></pre></p> </li> <li> <p>Enable and start the backend <pre><code>systemctl --user enable fusionpipe_backend.service\nsystemctl --user start fusionpipe_backend.service\nsystemctl --user status fusionpipe_backend.service\n</code></pre></p> </li> <li> <p>Enable and start the frontend <pre><code>systemctl --user enable fusionpipe_frontend.service\nsystemctl --user start fusionpipe_frontend.service\nsystemctl --user status fusionpipe_frontend.service\n</code></pre></p> </li> <li> <p>Let the user systemd be run when the user disconnect <pre><code>sudo loginctl enable-linger $USER\n</code></pre></p> </li> <li> <p>Stop services <pre><code>systemctl --user stop fusionpipe_frontend.service\nsystemctl --user stop fusionpipe_backend.service\n</code></pre></p> </li> <li> <p>kill processes if they don't stop by themselves. <pre><code>systemctl --user kill fusionpipe_frontend.service\n</code></pre></p> </li> </ul>"},{"location":"dev_maintainer_guide/#upgrade-the-production-version","title":"Upgrade the production version","text":"<ul> <li>Login with the `fusionpipe</li> <li>Navigate the <code>fusionpipeadmin</code> user.</li> <li>Pull the desired version of fusionpipe <pre><code>git pull\n</code></pre></li> <li> <p>Stop services <pre><code>systemctl --user stop fusionpipe_frontend.service\nsystemctl --user stop fusionpipe_backend.service\n</code></pre></p> </li> <li> <p>Navigate the frontend folder <pre><code>cd src/fusionpipe/frontend\n</code></pre></p> </li> <li> <p>Set the environment variable from from your production env. See section above</p> </li> <li> <p>Recompile the frontend <pre><code>npm run build\n</code></pre></p> </li> <li> <p>Start the systemd services <pre><code>systemctl --user start fusionpipe_backend.service\nsystemctl --user start fusionpipe_frontend.service\n</code></pre></p> </li> <li> <p>Check that the services are working properly <pre><code>systemctl --user status fusionpipe_backend.service\nsystemctl --user status fusionpipe_frontend.service\n</code></pre></p> </li> </ul>"},{"location":"dev_maintainer_guide/#onboard-a-new-user","title":"Onboard a new user","text":"<ul> <li>Create new user with <code>&lt;newusername&gt;</code> in postgres </li> </ul> <p><code>psql -U postgres -d fusionpipe_prod1 -c \"CREATE USER &lt;newusername&gt;;\"</code></p> <ul> <li>Grant user access to role <code>fusionpipeusers</code> in postgres database</li> </ul> <p><code>psql -U postgres -d fusionpipe_prod1 -c \"GRANT fusionpipeusers TO &lt;newusername&gt;;\"</code></p> <ul> <li>Ask user to write the following line in the <code>.profile</code> or <code>.bashrc_profile</code>, in order to persist the access to the database when it log in.</li> </ul> <pre><code>export DATABASE_URL=\"dbname=fusionpipe_prod1 port=5432\"\n</code></pre> <ul> <li> <p>Write the location of the user utils in the <code>.profile</code>, <code>.bashrc_profile</code>. <pre><code>export USER_UTILS_FOLDER_PATH=\"&lt;absolute/path/to/user/utils&gt;\"\n</code></pre></p> </li> <li> <p>Add user to the group in the server, in order for him to have access to the shared repository <pre><code>sudo usermod -aG fusionpipeusers username\n</code></pre></p> </li> </ul>"},{"location":"dev_maintainer_guide/#user-set-up","title":"User set-up","text":"<ul> <li> <p>Communicate the username to the admin <pre><code>whoami\n</code></pre></p> </li> <li> <p>Install <code>uv</code> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p> </li> <li> <p>ssh to the machine and forward the port with the frontend and backend. Ask admin to tell the port used for deployment.  <pre><code>ssh -L &lt;fronend_port&gt;:localhost:&lt;fronend_port&gt; -L &lt;backend_port&gt;:localhost:&lt;backend_port&gt; &lt;username&gt;@&lt;host&gt;\n</code></pre></p> </li> <li> <p>Write the following line in the <code>.profile</code>. This allows the user to have access to the database <pre><code>export DATABASE_URL=\"dbname=fusionpipe_prod1 port=5432\"\n</code></pre></p> </li> <li> <p>Write the location of the user utils in the <code>.profile</code> <pre><code>export USER_UTILS_FOLDER_PATH=\"&lt;absolute_path_to_user_utils&gt;\"\n</code></pre></p> </li> <li> <p>When switching to a node run, if you want to user jupyter notebooks, run the following command to initialise the python kernel. <pre><code>uv run python init_node_kernel.py\n</code></pre></p> </li> </ul>"},{"location":"dev_maintainer_guide/#set-up-docker-at-system-level-require-sudo","title":"Set up docker at system level. (require sudo)","text":"<p>Or ask your admin to do that for you.</p> <pre><code>sudo apt-get update\nsudo apt-get install docker.io\n</code></pre> <p>Start docker service at system level <pre><code>sudo systemctl enable docker\nsudo systemctl start docker\n</code></pre></p> <p>Add your user to docker <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p>Re-start your user session, and check your user is in the docker group <pre><code>groups\n</code></pre></p> <p>Check if docker compose is installed <pre><code>docker compose version\n</code></pre></p> <p>Otherwise install it with <pre><code>pip install docker-compose\n</code></pre></p> <p>Docker can load many images, that can occopy space in your disk. You might want to consider to set the location where docker images are saved.</p>"},{"location":"installation_single/","title":"Single-User Installation Guide","text":"<p>This guide provides instructions for installing <code>fusionpipe</code> for a local, single-user setup, ideal for individual users and developers.</p>"},{"location":"installation_single/#introduction","title":"Introduction","text":"<p><code>fusionpipe</code> is a lightweight pipeline orchestrator designed to help organize data analysis, simulation, and machine learning pipelines. It consists of the following components:</p> <ul> <li>A frontend developed in Svelte.</li> <li>A backend web app using FastAPI.</li> <li>A PostgreSQL database to track relationships between nodes and pipelines.</li> </ul> <p>Warning</p> <p>This guide assumes you have <code>sudo</code> access on your machine. If not, you may need to contact your IT administrator for assistance with PostgreSQL setup.</p> <p>This guide is for a bare-metal Unix-like system. A Docker-based installation is currently under development.</p>"},{"location":"installation_single/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>fusionpipe</code>, ensure you have the following prerequisites installed on your system.</p>"},{"location":"installation_single/#postgresql","title":"PostgreSQL","text":"<p><code>fusionpipe</code> uses PostgreSQL to store metadata.</p> <ol> <li> <p>Install PostgreSQL     Follow the official guidelines for your operating system from the PostgreSQL website.</p> </li> <li> <p>Configure the Database     The following commands will create the necessary databases and grant permissions to your user.</p> <ul> <li> <p>Log in as the <code>postgres</code> user:     <pre><code>sudo -u postgres -i\n</code></pre></p> </li> <li> <p>Create the main database:     <pre><code>createdb fusionpipe_dev\n</code></pre></p> </li> <li> <p>Grant permissions to your OS user (replace <code>&lt;myuser&gt;</code> with your actual username):     <pre><code>psql -U postgres -c \"CREATE ROLE &lt;myuser&gt;;\"\npsql -U postgres -d fusionpipe_dev -c \"GRANT CONNECT ON DATABASE fusionpipe_dev TO &lt;myuser&gt;;\"\npsql -U postgres -d fusionpipe_dev -c \"GRANT ALL ON SCHEMA public TO &lt;myuser&gt;;\"\npsql -U postgres -d fusionpipe_dev -c \"ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO &lt;myuser&gt;;\"\npsql -U postgres -d fusionpipe_dev -c \"ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO &lt;myuser&gt;;\"\npsql -U postgres -d fusionpipe_dev -c \"GRANT CREATE ON DATABASE fusionpipe_dev TO &lt;myuser&gt;;\"\npsql -U postgres -c \"ALTER ROLE &lt;myuser&gt; CREATEDB;\"\n</code></pre></p> </li> <li> <p>Create a database for tests:     <pre><code>createdb fusionpipe_test\n</code></pre></p> </li> <li> <p>Grant permissions for the test database:     <pre><code>psql -U postgres -d fusionpipe_test -c \"GRANT ALL PRIVILEGES ON DATABASE fusionpipe_test TO &lt;myuser&gt;;\"\n</code></pre></p> </li> </ul> </li> </ol> <p>PostgreSQL Access Control</p> <p>By default, PostgreSQL uses <code>peer</code> authentication, which authenticates users based on their OS username. <code>fusionpipe</code> assumes this default behavior. If you need to use a different authentication method (e.g., password-based), you will need to modify the <code>pg_hba.conf</code> file, typically located at <code>/var/lib/pgsql/data/pg_hba.conf</code>.</p>"},{"location":"installation_single/#uv-python-package-manager","title":"uv (Python Package Manager)","text":"<p><code>fusionpipe</code> uses uv for Python package management. Install it using <code>pip</code>: <pre><code>pip install uv\n</code></pre></p>"},{"location":"installation_single/#nodejs-and-npm","title":"Node.js and npm","text":"<p>The frontend is a Svelte application, which requires Node.js and npm. Install them by following the official instructions at nodejs.org.</p>"},{"location":"installation_single/#installation","title":"Installation","text":"<ol> <li> <p>Clone the Repository <pre><code>git clone &lt;fusionpipe_url&gt;\n</code></pre></p> </li> <li> <p>Navigate to the Project Directory <pre><code>cd fusionpipe\n</code></pre></p> </li> <li> <p>Install Dependencies</p> <p>Use <code>uv</code> to install the required Python packages: <pre><code>uv sync\n</code></pre></p> </li> </ol>"},{"location":"installation_single/#configuration","title":"Configuration","text":""},{"location":"installation_single/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the root of the project to store the necessary environment variables.</p> <pre><code># .env\nFUSIONPIPE_DATA_PATH=\"&lt;absolute/path/to/fusionpipe/data/folder&gt;\"\nUV_CACHE_DIR=\"&lt;absolute/path/to/uv/cache/dir&gt;\"\nDATABASE_URL=\"dbname=fusionpipe_dev port=5432\"\nDATABASE_URL_TEST=\"dbname=fusionpipe_test port=5432\"\n\nBACKEND_HOST=\"localhost\"\nBACKEND_PORT=8000\n\nVITE_BACKEND_HOST=${BACKEND_HOST}\nVITE_BACKEND_PORT=${BACKEND_PORT}\nVITE_FRONTEND_HOST=\"localhost\"\nVITE_FRONTEND_PORT=5173\n\nUSER_UTILS_FOLDER_PATH=\"&lt;absolute/path/to/user/utils&gt;\"\nFP_MATLAB_RUNNER_PATH=\"&lt;absolute/path/to/matlab/executable&gt;\"\nVIRTUAL_ENV=\"&lt;(optional) default virtual env to activate when running the backend&gt;\"\n</code></pre> <p>Here\u2019s an explanation of the variables:</p> <ul> <li><code>FUSIONPIPE_DATA_PATH</code>: Absolute path to the folder where <code>fusionpipe</code> will store its data.</li> <li><code>UV_CACHE_DIR</code>: Path to the cache directory for <code>uv</code>.</li> <li><code>DATABASE_URL</code>: Connection string for the main PostgreSQL database. The example assumes <code>peer</code> authentication.</li> <li><code>DATABASE_URL_TEST</code>: Connection string for the test database.</li> <li><code>BACKEND_PORT</code>: Port for the FastAPI backend (e.g., 8000).</li> <li><code>VITE_FRONTEND_PORT</code>: Port for the Svelte frontend (e.g., 5173).</li> <li><code>USER_UTILS_FOLDER_PATH</code>: Absolute path to a folder for shared user utilities.</li> <li><code>FP_MATLAB_RUNNER_PATH</code>: (Optional) Absolute path to your MATLAB executable if you plan to use it.</li> </ul> <p>Warning</p> <p>If your PostgreSQL setup uses password authentication, you must update the <code>DATABASE_URL</code> and <code>DATABASE_URL_TEST</code> variables accordingly: <code>DATABASE_URL=\"dbname=&lt;db_name&gt; user=&lt;user&gt; password=&lt;pass&gt; host=&lt;host&gt; port=&lt;port&gt;\"</code></p>"},{"location":"installation_single/#getting-started","title":"Getting Started","text":"<p>To run <code>fusionpipe</code>, you need to start both the backend and frontend services in separate terminals.</p>"},{"location":"installation_single/#start-the-backend","title":"Start the Backend","text":"<ol> <li>Open a new terminal.</li> <li>Navigate to the <code>fusionpipe</code> directory.</li> <li>Load the environment variables: <pre><code>source .env\n</code></pre></li> <li>Navigate to the backend directory: <pre><code>cd src/fusionpipe/api\n</code></pre></li> <li>Start the backend service: <pre><code>uv run python main.py\n</code></pre> The backend will be available at <code>http://localhost:8000</code> (or the port you specified).</li> </ol>"},{"location":"installation_single/#start-the-frontend","title":"Start the Frontend","text":"<ol> <li>Open another new terminal.</li> <li>Navigate to the <code>fusionpipe</code> directory.</li> <li>Load the environment variables: <pre><code>source .env\n</code></pre></li> <li>Navigate to the frontend directory: <pre><code>cd src/fusionpipe/frontend\n</code></pre></li> <li>Install npm dependencies: <pre><code>npm install\n</code></pre></li> <li>Start the frontend service in development mode: <pre><code>npm run dev\n</code></pre></li> </ol> <p>You can now access the <code>fusionpipe</code> frontend in your browser at <code>http://localhost:5173</code> (or the port you specified).</p>"},{"location":"intro/","title":"Getting Started","text":"<p><code>fusionpipe</code> is a lightweight pipeline orchestrator designed to streamline data analysis, simulations, and machine learning workflows, fostering better collaboration among users. It enables rapid prototyping with minimal interface complexity while scaling seamlessly to full production systems.</p> <p>The guiding principle behind <code>fusionpipe</code> is that \"fast iteration is key to data science exploration.\" As a result, the backend interface is intentionally kept minimal, allowing you to develop your code with as little overhead as possible\u2014just as you normally would.</p> <p><code>fusionpipe</code> consists of few core components:</p> <ul> <li>Node</li> <li>Pipeline</li> </ul>"},{"location":"intro/#node","title":"Node","text":"<p>A node is a directory with a unique <code>&lt;node_id&gt;</code> that adheres to the following minimal structure:</p> <pre><code>&lt;node_id&gt;/\n\u251c\u2500\u2500 code/\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u251c\u2500\u2500 logs.txt\n</code></pre> <ul> <li>The <code>code</code> folder contains the node's source code, with <code>main.py</code> serving as the entry point.</li> <li>The <code>data</code> folder stores output results generated by the node.</li> <li>The <code>logs.txt</code> file records execution logs.</li> </ul> <p>The <code>main.py</code> script is executed by the pipeline when the node runs. </p> <p>Being able to access data from your parent node is the only interface that you need integrate convert your code into a pipeline, and convenience user APIs are provided for that.</p> <p>A node may include calls to Python scripts, Jupyter notebooks, MATLAB scripts, or other executable code.</p>"},{"location":"intro/#pipeline","title":"Pipeline","text":"<p>A pipeline is a directed acyclic graph (DAG) that connects multiple nodes. Each node can have multiple parent and child nodes, enabling the creation of complex workflows. The pipeline orchestrator handles node execution based on dependencies, ensuring that parent nodes are processed before their children.</p> <p></p>"},{"location":"intro/#start-using-it","title":"Start using it","text":"<p>Depending on your role there are several way to can get started with fusionpipe:</p> <ul> <li>user on managed instance: In this case <code>fusionpipe</code> is already availabe as a service in your machine/VM. Ask your maintainer how access and follow user guide to develop your first node.</li> <li>user with local installation/developer: In this case you are going to install <code>fusionpipe</code> on your machine to either use it personally or contribute to it. Follow the single user installation guidelines</li> <li>maintainer: In this case your are installing <code>fusionpipe</code> your your barematal cluster as administrator for multiple to allow access to multiple users. Follow the multiple users installation guidelines</li> </ul>"},{"location":"package_management_matlab/","title":"matlab","text":"<p>In the following the guideline the instructions to include external MATLAB code in your pipeline is presented in details.</p>"},{"location":"package_management_matlab/#preliminaries","title":"Preliminaries","text":""},{"location":"package_management_matlab/#difference-between-matlab-script-and-matlab-packagestoolboxes","title":"Difference between MATLAB script and MATLAB packages/toolboxes","text":"<p>A MATLAB script is a single <code>.m</code> file containing MATLAB code, typically used for small tasks, automation, or quick experiments. Scripts are usually not structured for reuse and are executed directly.</p> <p>A MATLAB package or toolbox is a collection of functions and classes organized in directories with a <code>+</code> prefix (for packages) or as a structured toolbox. Packages are designed for code reuse, distribution, and maintainability, making it easier to share and manage complex codebases across multiple projects. Most of the toolboxes you install from MATLAB File Exchange or MathWorks are MATLAB packages/toolboxes, and you typically call them using statements like <code>packagename.functionname()</code> or directly if they're on the MATLAB path.</p>"},{"location":"package_management_matlab/#manage-dependencies","title":"Manage dependencies","text":"<p>There can be 2 situations in this case:</p> <ul> <li><code>Developer</code>: You are modifying the package/toolbox while you develop the pipeline.</li> <li><code>User</code>: The package/toolbox remains the same during the development of the full pipeline.</li> </ul> Developer <p>In this case the code in the MATLAB package/toolbox evolves while you are developing the pipeline. This is the most common case when you are working on your personal project. Usually your code is version controlled with <code>.git</code> independently from your pipeline.</p> <p>There are two possible solutions that you can adopt to integrate your code in one node of your pipeline.</p> a) Every node has an independent snapshot of the external package <p>In this case in every node a snapshot of your code is available.  This means that you either copy paste your code from your local location, or you clone the repository from remote <code>.git</code>.</p> <pre><code>git clone ./&lt;url/mypackage&gt;\n</code></pre> <p>You can then add your package to the MATLAB path of the node by:</p> <ol> <li>Placing the package in the node's <code>code</code> directory</li> <li>Adding the package directory to MATLAB's path in your main MATLAB script: <pre><code>addpath(genpath('./mypackage'));\n</code></pre></li> </ol> <p>You will be able then to call functions from your package in your MATLAB code as usual: <pre><code>% For regular functions\nresult = myfunction(input);\n\n% For package functions\nresult = mypackage.myfunction(input);\n</code></pre></p> <p>This approach ensures that changes you make to your external package source code are immediately available when MATLAB is executed in the node.</p> <p>Warning</p> <p>Whenever you create a new node, you will need to follow these steps for all the external packages that are needed in your node. When you duplicate a node instead, the new node will be ready to be used.</p> <p>Info: Best practice to update the external packages in previously created nodes</p> <p>Suppose you have already started developing your pipeline, and your external package continues to evolve during development. Each node you create and run will contain a snapshot of the external package as it existed at the time the node was executed. If you later want to update the package for an existing (older) node\u2014especially if changes to the package are expected to affect the node\u2019s output\u2014you may need to update the package in that node and re-run it, along with any downstream (child) nodes.</p> <p>In case you are sure that the changes of your code will not change the outputs of the nodes:</p> <ul> <li>Navigate the node folder, <code>cd node_folder_path</code>.</li> <li>Navigate your package subfolder.</li> <li>Merge the latest version of your package <code>git pull</code>.</li> </ul> <p>If case the changes of your package can affect the output of the node:</p> <ul> <li>Duplicate the node (without data)</li> <li>Attach the nodes to the same parents as for the original one.</li> <li>Navigate the node folder, <code>cd node_folder_path</code>.</li> <li>Merge the latest version of your package <code>git pull</code>.</li> <li>Rerun the node.</li> <li>If completed successfully, swap the node with the one containing the new package in the pipeline.</li> </ul> <p>Tip: Create a template node with all your external packages and duplicate it</p> <p>When multiple external packages are needed in the same node, in order to avoid to <code>git pull</code> all of them for every node, you can create a node that you will use as a template. In this node you can pull and add all the dependencies. Then, you can duplicate this node instead of creating a new one and add dependencies.</p> b) Every node shares the same instance of the external package <p>In this case your package is located in 1 single location on your disk. Every node will reference to this package.</p> <p>Tip</p> <p>It is recommended to create a node in your pipeline which will only contain the package that will be shared with the other nodes. This will allow all the users that are collaborating on your pipeline to have access to the code. Also, this will allow you to have your pipeline be self-contained.</p> <p>When you create a new node:</p> <ul> <li>Navigate the code folder <code>node_id/code</code></li> <li>Add the package path to your MATLAB script using the absolute path to the location where the package is located: <pre><code>addpath(genpath('/absolute/path/to/package'));\n</code></pre></li> </ul> <p>When you duplicate a node where you referenced an external package, this will be ready to use. No need to re-add the package path.</p> <p>There are pros and cons with both solutions:</p> <ul> <li> <p>a) Every node has an independent snapshot of the external package</p> <ul> <li>PROS:<ul> <li>Each node is self-contained and portable.</li> <li>Changes to the package in one node do not affect others.</li> <li>Easier to reproduce results for a specific node at a given point in time.</li> </ul> </li> <li>CONS:<ul> <li>Requires manual updates of the package in each node if changes are made.</li> <li>Can lead to code duplication.</li> <li>More maintenance overhead when managing multiple nodes.</li> </ul> </li> </ul> </li> <li> <p>b) Every node shares the same instance of the external package</p> <ul> <li>PROS:<ul> <li>Centralized management of the package; updates are immediately available to all nodes.</li> <li>Reduces code duplication and saves disk space.</li> <li>Easier to maintain consistency across nodes.</li> </ul> </li> <li>CONS:<ul> <li>Changes to the package can affect all nodes, potentially breaking reproducibility.</li> <li>Path conflicts might occur if different nodes require different versions.</li> </ul> </li> </ul> </li> </ul> <p>Tip</p> <p>It is recommended to use option a) while developing your pipeline. Pipelines often evolve over time, becoming complex as you iterate on your code, and many nodes may require significant compute resources to process full datasets. The extra effort to update packages in old nodes with solution a) is usually outweighed by the benefit of avoiding hard-to-debug issues caused by breaking changes and loss of reproducibility that can occur with solution b).</p> User <p>In this case, the external package does not change while you are developing the pipeline. This is typical when the MATLAB package comes from a stable distribution such as MATLAB File Exchange, a released toolbox, or a remote repository (e.g., GitHub or GitLab).</p> <p>To add such a package to your node:</p> <ul> <li>Navigate to the node's code folder: <code>cd node_id/code</code></li> <li>Add the package using one of the following methods:</li> </ul> Package from MATLAB File Exchange <ol> <li>Download the package from MATLAB File Exchange</li> <li>Extract it to your node's <code>code</code> directory or a subdirectory</li> <li>Add the package to MATLAB's path in your script: <pre><code>addpath(genpath('./downloaded_package'));\n</code></pre></li> </ol> Package hosted on GitHub/GitLab or another remote repository <p>You can clone the repository directly into your node. For example: <pre><code>cd node_id/code\ngit clone https://github.com/username/repository.git\n</code></pre> Then add it to MATLAB's path: <pre><code>addpath(genpath('./repository'));\n</code></pre></p> <p>Tip</p> <p>For better reproducibility and to ensure all nodes use the same version of the package, it is recommended to:</p> <ol> <li>Create a dedicated node to download and store the package locally.</li> <li>Clone or download the package into this node at a specific commit/tag.</li> <li>Reference the package in other nodes by providing the absolute path to the local copy. For example:     <pre><code>addpath(genpath('/absolute/path/to/package'));\n</code></pre> This ensures all nodes reference the same package version, avoiding inconsistencies due to remote updates or changes.</li> </ol> <p>If your MATLAB code has dependencies on specific toolboxes or external libraries, document them clearly in your node's README or in comments within your MATLAB scripts. For external libraries (non-MATLAB), consider creating installation scripts or providing clear setup instructions within the node.</p>"},{"location":"package_management_python/","title":"python","text":"<p>In the following the guideline the instructions to include external python code in yout pipeline is presented in details.</p>"},{"location":"package_management_python/#preliminaries","title":"Preliminaries","text":""},{"location":"package_management_python/#difference-between-python-script-and-python-packages","title":"Difference between python script and python packages","text":"<p>A Python script is a single file containing Python code, typically used for small tasks, automation, or quick experiments. Scripts are usually not structured for reuse and are executed directly.</p> <p>A Python package is a collection of modules organized in directories with an <code>__init__.py</code> file. Packages are designed for code reuse, distribution, and maintainability, making it easier to share and manage complex codebases across multiple projects. Most of the libraries you install from PyPI (using <code>pip install ...</code>) are Python packages, and you typically import them using statements like <code>from package_name import ...</code>.</p> <p>It is highly recommended to package your code into a Python package. Follow the guide in Best Practices for Pipeline Packages. However, in the following instructions are provided for both cases where you python code is a script or python package</p>"},{"location":"package_management_python/#virtual-environment-for-a-node","title":"Virtual environment for a node","text":"<p>Every node is initialise with the command <code>uv init</code>, which automatically create a dedicated python virtual environment which will contains the dependencies for the node. This can be activate from the 'node_id/code' subfolder</p> <p><pre><code>cd code\nsource .venv/bic/activate\n</code></pre> See guideline at page Delevelop a Node</p>"},{"location":"package_management_python/#extarnal-code-is-a-python-packaged","title":"Extarnal code is a python packaged","text":"<p>There can be 2 situations in this case. </p> <ul> <li><code>Developer</code>: Your are modifying the package while you develop the pipeline.</li> <li><code>User</code>: The package remains the same during the development of the full pipeline.</li> </ul> Developer <p>In this case the code in the python package evolves while you are developing the pipeline. This is the most common case when you are working on your personal project. Usually your code, is version controller <code>.git</code> independently from your pipeline.</p> <p>There are two possibile solutions that you can adopt to integrate your code in one node of your pipeline.</p> a) Every node has an independent snapshot of the external package <p>In this case in every node a snapshot of your code is available.  This means that you either copy paste your code from your local location, or you clone the repository from remote <code>.git</code>. </p> <pre><code>git clone ./&lt;url/mypackageo&gt;\n</code></pre> <p>You can then add your repo to the <code>.venv</code> of the node with  <pre><code>uv add --editable &lt;mypackage&gt;\n</code></pre> You will be able then to import functions from your package into the <code>main.py</code> function as usual <code>from mypackage import *</code>.</p> <p>The editable flag ensures that changes you make to your external package source code are immediately reflected in the node\u2019s environment without needing to reinstall the package each time. This way you can import a function on the <code>main.py</code> from your package, and modification of your package will be immediately available. </p> <p>This step will also automatically install all the dependencies of the external python package in the <code>.venv</code> of your node.</p> <p>Warning</p> <p>Whenever you create a new node, you will need to follow these steps for all the external packages that are needed in your node. When you duplicate a node instead, the new node will be ready ready to be used with all the dependencies installed.</p> <p>Info: Best practise to update the external packages in previously created nodes</p> <p>Suppose that you started developing your pipeline, and your external packages evolves changes while you develop the pipeline. A node that was created and run in the past will have a snapshot of the the external package as on the moment that the node has run. It could happen that you would like to updated the package for an old node, and eventually re-run the node and its children if the changes are expected to affect the output of the node.</p> <p>In case you are sure that the changes of your code will not change the outputs of the nodes: - Navigate the node folder, <code>cd node_folder_path</code>. - Navigate your package subfolder. - Merge the latest version of your package <code>git pull</code>.</p> <p>If case the changes of your package can affect the output of the node:  - Duplicate the node (without data) - Attach the nodes to the same parents as for the original one. - Navigate the node folder, <code>cd node_folder_path</code>. - Merge the latest version of your package <code>git pull</code>. - Rerun the node. - If completed succesfully, swap the node with the one containing the new package in the pipeline.</p> <p>Tip: Create a template node with all your external packages and duplicate it</p> <p>When multiple external packages are needed in the same node, in order to avoid to <code>git pull</code> all of them for every node, you can create a node that you will use as a template. In this node you can pull and add all the dependencies. Then, you can duplicate this node instead of creating a new one and add depencies.</p> <p>Info</p> <p><code>uv</code> will not re-download, or duplicate in the disk dependencies if those are already available in the <code>uv</code> cache. This is convenient especially for large (&gt;1Gib) packages like <code>pytorch</code>,<code>tensorflow</code> to avoid wasting disk space.</p> b) Every node share the same instance of the external package <p>In this case your package is located in 1 single location on your disk. Every node will reference to this package.</p> <p>Tip</p> <p>It is recommended to create a node in your pipeline which will only contain the package that will be shared with the other nodes. This will allow all the users that are collaborating on your pipeline to have access to the code. Also, this will allow you to have your pipeline be self-contained.</p> <p>When you creaet a new node:</p> <ul> <li>Navigate the code folder <code>code_id/node</code></li> <li>Add the package using the absolute path to the location on this where the package is located <pre><code>uv add --editable &lt;absolute/path/to/package&gt;\n</code></pre></li> </ul> <p>When you duplicate a node where you referenced an external package, this will be ready to use. No need to re-add the package. There are pros and cons with both solutions:</p> <p>There are pros and cons with both solutions:</p> <ul> <li> <p>a) Every node has an independent snapshot of the external package</p> <ul> <li>PROS:<ul> <li>Each node is self-contained and portable.</li> <li>Changes to the package in one node do not affect others.</li> <li>Easier to reproduce results for a specific node at a given point in time.</li> </ul> </li> <li>CONS:<ul> <li>Requires manual updates of the package in each node if changes are made.</li> <li>Can lead to code duplication.</li> <li>More maintenance overhead when managing multiple nodes.</li> </ul> </li> </ul> </li> <li> <p>b) Every node shares the same instance of the external package</p> <ul> <li>PROS:<ul> <li>Centralized management of the package; updates are immediately available to all nodes.</li> <li>Reduces code duplication and saves disk space.</li> <li>Easier to maintain consistency across nodes.</li> </ul> </li> <li>CONS:<ul> <li>Changes to the package can affect all nodes, potentially breaking reproducibility.</li> </ul> </li> </ul> </li> </ul> <p>Tip</p> <p>It is recommended to use option a) while developing your pipeline. Pipelines often evolve over time, becoming complex as you iterate on your code, and many nodes may require significant compute resources to process full datasets. The extra effort to update packages in old nodes with solution a) is usually outweighed by the benefit of avoiding hard-to-debug issues caused by breaking changes and loss of reproducibility that can occur with solution b).</p> User <p>In this case, the external package does not change while you are developing the pipeline. This is typical when the Python package comes from a stable distribution such as PyPI (e.g., <code>numpy</code>) or a remote repository (e.g., GitHub or GitLab).</p> <p>To add such a package to your node:</p> <ul> <li>Navigate to the node's code folder: <code>cd node_id/code</code></li> <li>Add the package using:   <pre><code>uv add &lt;package-location&gt;\n</code></pre></li> </ul> Package hosted in PyPI <p>Simply specify the package name. For example: <pre><code>uv add numpy\n</code></pre> To specify a particular version of a package from PyPI, append <code>==&lt;version&gt;</code> to the package name. For example, to install version 1.24.0 of numpy:</p> <pre><code>uv add numpy==1.24.0\n</code></pre> <p>This ensures that the specified version is installed in your node's environment.</p> Package hosted on GitHub/Gitlab or another remote repository <p>You can provide the repository URL directly. For example: <pre><code>uv add git+https://github.com/username/repository.git\n</code></pre> This will install the package from the remote source.</p> <p>Tip</p> <p>For better reproducibility and to ensure all nodes use the same version of the package, it is recommended to:</p> <ol> <li>Create a dedicated node to download and store the package locally.</li> <li>Download or clone the package into this node.</li> <li>Add the package to other nodes by providing the absolute path to the local copy. For example:     <pre><code>uv add --editable /absolute/path/to/package\n</code></pre> This ensures all nodes reference the same package version, avoiding inconsistencies due to remote updates or changes.</li> </ol>"},{"location":"package_management_python/#external-code-is-only-a-collection-of-python-scripts","title":"External code is (only) a collection of Python scripts","text":"<p>Tip</p> <p>For maximum reproducibility and maintainability, it is best to organize your scripts as a Python package rather than as standalone scripts. Packaging your code ensures consistent imports, easier dependency management, and better compatibility with pipeline tools. See Best Practices for Pipeline Packages for guidance on structuring your code as a package.</p> <p>There are two main scenarios when including standalone Python scripts (not organized as a package) in your pipeline:</p> Developer <p>If you are actively developing or modifying your scripts while building your pipeline, consider the following approaches:</p> a) Copy scripts into each node <ul> <li>Place your script(s) directly in the <code>code</code> subfolder of the node.</li> <li>Import functions or classes from these scripts in your <code>main.py</code> using standard Python import statements:     <pre><code>from my_script import my_function\n</code></pre></li> <li>PROS: Each node is self-contained and reproducible. Changes to scripts in one node do not affect others.</li> <li>CONS: Manual updates are needed if scripts change, and code duplication may occur across nodes.</li> </ul> <p>Tip</p> <p>If you update your script and want to propagate changes to other nodes, you will need to manually copy the updated script into each relevant node.</p> b) Reference scripts from a shared location <ul> <li>Store your scripts in a shared directory outside the node folders.</li> <li>In your <code>main.py</code>, add the path to this directory to <code>sys.path</code> at runtime:     <pre><code>import sys\nsys.path.append('/absolute/path/to/shared/scripts')\nfrom my_script import my_function\n</code></pre></li> <li>PROS: Centralized management; updates are immediately available to all nodes.</li> <li>CONS: Changes to the script affect all nodes, which may impact reproducibility.</li> </ul> <p>Warning</p> <p>Be cautious: updating a shared script will affect all nodes that reference it, potentially breaking previous results.</p> c) Use symbolic links (advanced) <ul> <li>Create a symbolic link in the node's <code>code</code> folder pointing to the shared script location:     <pre><code>ln -s /absolute/path/to/shared/scripts/my_script.py code/my_script.py\n</code></pre></li> <li>PROS: Keeps node folders organized and avoids duplication.</li> <li>CONS: Like referencing a shared location, changes to the script affect all nodes using the link.</li> </ul> <p>Tip</p> <p>For most development workflows, copying scripts into each node is recommended for reproducibility. Use shared references or symbolic links if you prioritize easier maintenance and are aware of the implications for reproducibility.</p> User <p>If your scripts are stable and will not change during pipeline development (for example, they are provided by a third party or are finalized):</p> <ul> <li>It is recommended to create a dedicated node in your pipeline to store the shared scripts in a read-only location.</li> <li>In each node that requires these scripts, add the shared scripts directory to <code>sys.path</code> in your <code>main.py</code>:     <pre><code>import sys\nsys.path.append('/absolute/path/to/shared/scripts')\nfrom my_script import my_function\n</code></pre></li> <li>Alternatively, you may create symbolic links in the node's <code>code</code> folder pointing to the shared scripts.</li> </ul> <p>This approach ensures all nodes reference the same, unchanging version of the script, supporting reproducibility and easier maintenance.</p> <p>If your scripts have dependencies, install them in the node's virtual environment using <code>uv add &lt;package&gt;</code> as described in the previous sections.</p> <p>Alternatively, if your dependencies are listed in a <code>requirements.txt</code> file, you can install all of them at once by running:</p> <pre><code>uv pip install -r requirements.txt\n</code></pre> <p>This will install every package specified in the <code>requirements.txt</code> file into the node's virtual environment.</p>"},{"location":"developer_guide/design_principles/","title":"Design Principles","text":"<p>TODO</p>"},{"location":"user_guide/complete_list_interaction/","title":"Complete list of functions","text":""},{"location":"user_guide/complete_list_interaction/#project-interaction","title":"Project Interaction","text":"<p>Different pipelines can be grouped in a project.</p> <p><code>List ids</code>: use this option to search and select a project by its unique identifier.</p> <p><code>List tags</code>: use this option to search and select a project by its unique tag.</p> <p><code>Select a project</code>: this option displays a list of available projects, ordered by either id or tag based on your previous selection. From this list, you can choose the project you wish to open.</p> <p><code>Open selected project panel</code>: this panel allows you to change the project tag and add notes that may be useful for you and future project users.</p> <p><code>Create project</code>: select this option to create a new project.</p> <p><code>Delete project</code>: select this option to delete the actual project.</p>"},{"location":"user_guide/complete_list_interaction/#pipeline-interaction","title":"Pipeline Interaction","text":"<p><code>List ids</code>: use this option to search and select a pipeline by its unique identifier.</p> <p><code>List tags</code>: use this option to search and select a pipeline by its unique tag.</p> <p><code>Select a pipeline</code>: this option displays a list of available pipelines, ordered by either id or tag based on your previous selection. From this list, you can choose the pipeline you wish to open.</p> <p><code>Open selected pipeline panel</code>: this panel allows you to change the pipeline tag and add notes that may be useful for you and future pipelines users.</p> <p><code>Create Pipeline</code>: select this option to create a new pipeline.</p> <p><code>Branch Pipeline from selected node</code>: select this option to delete the actual pipeline.</p> <p><code>Move Pipeline to project</code>: use this command to move a pipeline into a different project. Click on <code>Select a project</code>, choose the destination project, and then click <code>Move</code> to transfer the pipeline.</p> <p><code>Merge pipelines</code>: use this command to combine multiple pipelines into a single new pipeline. Click on <code>Select multiple pipelines</code>, choose the pipelines you wish to merge, and then select <code>Merge</code>. The resulting pipeline will include a unique copy of all nodes that are shared between the selected pipelines, as well as all nodes that are unique to each pipeline. This ensures that no data or workflow steps are lost during the merge process and the new pipeline provides a comprehensive view of all included nodes.</p> <p><code>Delete Pipeline</code>: select this option to delete the actual pipeline.</p>"},{"location":"user_guide/complete_list_interaction/#node-interaction","title":"Node Interaction","text":"<p><code>Open selected node panel</code>: this panel allows you to change the pipeline tag and add notes that may be useful for you and future pipelines users. You can fill the <code>Node Parameters (YAML)</code> box, which will store your parmeters in a YAML file to be used as fixed parameteres inputs in the node code.</p> <p><code>Copy selected node path to clipboard</code>: select a node and use this option to copy its folder path to the clipboard. Alternatively, you can right-click on the node and choose the \"Copy Folder Path\" option from the context menu for a quicker solution.</p> <p><code>Create node</code>:  select this option to create a new node.</p> <p><code>Duplicate selected nodes into this pipeline</code>: duplicate the selected nodes into the pipeline you are working on with or without data (<code>Duplicate with data</code>, <code>Duplicate without data</code>).</p> <p><code>Duplicate selected nodes into another pipeline</code>: duplicate the selected nodes into a different pipeline with or without data (<code>Duplicate with data</code>, <code>Duplicate without data</code>).</p> <p><code>Reference selected nodes into another pipeline</code>: reference the selected nodes into a different pipeline.</p> <p><code>Manual set node \"completed\"</code>: use this option to manually set the selected node to the completed state. This can be useful for marking a node as finished when its output is already available or when you want to bypass execution for testing or troubleshooting purposes.</p> <p><code>Manual set node \"stale-data\"</code>: use this option to manually set the selected node to the stale-data state. It can be useful if the user has changed the code in a node. Then it can flag the node to be in stale-data state. This way the pipeline needs to re-run the node and all its children.</p> <p><code>Delete output selected nodes</code>: use this command to clean the output data folder of a selected node.</p> <p><code>Delete selected edge</code>: use this option to delete a connection between two nodes.</p> <p><code>Delete selected nodes</code>: use this command to delete a selected node.</p>"},{"location":"user_guide/complete_list_interaction/#node-interaction_1","title":"Node Interaction","text":"<p><code>Run selected node</code>: this function will run the code of a selected node, setting its state to running state and at the end of the process to completed or failed state.</p> <p><code>Run full pipeline</code>: use this command to run the full pipeline.</p> <p><code>Run pipeline up to selected node</code>: select a node and click on this option to run the pipeline up to the selected node.</p> <p><code>Open run panel</code>: ...</p> <p><code>Kill run selected node</code>: select a node and choose this option to kill its execution.</p>"},{"location":"user_guide/complete_list_interaction/#layout","title":"Layout","text":"<p><code>Refresh pipeline</code>: click this command to refresh the state of pipeline nodes.</p> <p><code>Auto reshape</code>: use this option to automatically arrange the nodes in your pipeline for optimal organization and improved visualization.</p>"},{"location":"user_guide/develop_node/","title":"Develop your Node","text":"<p>A node is a directory with a unique <code>&lt;node_id&gt;</code> that adheres to the following minimal structure:</p> <pre><code>&lt;node_id&gt;/\n\u251c\u2500\u2500 code/\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u251c\u2500\u2500 logs.txt\n</code></pre> <ul> <li>The <code>code</code> folder contains the node's source code, with <code>main.py</code> serving as the entry point.</li> <li>The <code>data</code> folder stores output results generated by the node.</li> <li>The <code>logs.txt</code> file records execution logs.</li> </ul> <p>The <code>main.py</code> script is executed by the pipeline when the node runs. It can access data from parent nodes and may include calls to Python scripts, Jupyter notebooks, MATLAB scripts, or other executable code.</p>"},{"location":"user_guide/develop_node/#initial-setup","title":"Initial Setup","text":"<p>When a node is created from the UI, a dedicated Python virtual environment is automatically created using <code>uv</code>. This environment is located in a <code>.venv</code> folder inside your node's directory.</p> <p>To work on your node's code, you first need to navigate to its directory and activate the virtual environment.</p> <ol> <li> <p>Navigate to the node folder:</p> <p>You can copy the path from the GUI, then use it in your terminal: <pre><code>cd &lt;path_to_your_node&gt;\n</code></pre></p> </li> <li> <p>Activate the virtual environment:</p> <p>From within the node's directory, run: <pre><code>source .venv/bin/activate\n</code></pre> Your terminal prompt should now indicate that you are in the virtual environment.</p> </li> </ol>"},{"location":"user_guide/develop_node/#development-workflow","title":"Development Workflow","text":"<p>Here\u2019s a typical workflow for developing the logic for your node:</p> <ol> <li> <p>Develop your code:</p> <p>You can write your analysis, simulation, or machine learning code in a Jupyter notebook, a Python script, or even a MATLAB script. You can find examples in the <code>examples</code> directory of the project.</p> </li> <li> <p>Integrate with <code>main.py</code>:</p> <p>The <code>main.py</code> file is the entry point for your node's execution within the pipeline. You need to modify it to call the code you developed. The file already contains examples of how to call different types of scripts.</p> </li> <li> <p>Test your node locally:</p> <p>Before running the node as part of the full pipeline, you can test it in isolation. From your node's <code>code</code> directory, run: <pre><code>uv run python main.py\n</code></pre> This command uses the node's dedicated virtual environment to run your <code>main.py</code> script, simulating how the pipeline will execute it. Make sure you have all the necessary dependencies installed in the virtual environment.</p> </li> <li> <p>Run the node from the pipeline:</p> <p>Once you are satisfied with your local tests, you can run the node from the pipeline's user interface. This will execute the node in the correct order based on its dependencies.</p> </li> </ol>"},{"location":"user_guide/develop_node/#user-api","title":"User API","text":"<p>To access data from other nodes or manage the current node's data, <code>fusionpipe</code> provides a simple API. Here are the main functions you can use in your scripts:</p> <ul> <li> <p><code>get_node_id()</code>: Retrieves the ID of the current node. The ID follows the format <code>n_&lt;datetime&gt;_&lt;random_4digit_integers&gt;</code>.</p> </li> <li> <p><code>get_all_parent_node_folder_paths(node_id)</code>: Returns a list of folder paths for all parent nodes of the specified node. This is how you access the output data from the nodes that run before yours.</p> </li> <li> <p><code>get_folder_path_node()</code>: Retrieves the folder path of the current node. This is useful for saving your node's output to its <code>data</code> subfolder.</p> </li> </ul>"},{"location":"user_guide/develop_node/#using-jupyter","title":"Using Jupyter","text":"<p>If you prefer to develop using Jupyter Notebook or JupyterLab, you can set up a dedicated kernel for your node. This ensures that your notebook uses the same environment and dependencies as the pipeline.</p> <p>To create and set up the Jupyter kernel, navigate to your node's <code>code</code> directory and run: <pre><code>uv run python init_node_kernel.py\n</code></pre> This will create a new Jupyter kernel with the same name as your node's ID. You can then select this kernel in your Jupyter environment.</p> <p>For more advanced topics, such as developing a node in conjunction with a custom Python package, see the Best practices for pipeline and package development.</p>"},{"location":"user_guide/pipeline_interaction/","title":"Basic Interaction","text":""},{"location":"user_guide/pipeline_interaction/#basic-user-interaction","title":"Basic user interaction","text":"<p>In this section, you will learn the fundamentals of interacting with the FusionPipe frontend and how to effectively manage your data processing workflows.</p> <p> </p> <p>Follow these simple description:</p>"},{"location":"user_guide/pipeline_interaction/#create-a-pipeline","title":"Create a pipeline","text":"<p>First, click on <code>Pipeline Interaction</code> on the panel options shown in the image above and then on <code>Create Pipeline</code>. The pipeline has unique identifier, <code>pipeline_id</code>, and a <code>pipeline_tag</code>. The <code>pipeline_id</code>, differently then the <code>pipeline_tag</code>, cannot be modified by the user.  You can select the pipeline per ids (<code>List ids</code>) or tags (<code>List tags</code>) from the dropdown menu in <code>Pipeline Interaction</code>.</p>"},{"location":"user_guide/pipeline_interaction/#change-pipeline-name-and-add-notes","title":"Change pipeline name and add notes","text":"<p>From the <code>Pipeline Interaction</code> option, select <code>Open selected pipeline panel</code> to customize the pipeline tag with a meaningful name that helps you identify the specific purpose of your pipeline. This custom tag will allow you to easily search and access your pipeline later.</p> <p>Additionally, you can add detailed notes to document the pipeline's objectives, methodology, or any other relevant information for future reference.</p> <p></p>"},{"location":"user_guide/pipeline_interaction/#create-a-node","title":"Create a node","text":"<p>To create your first node, click on <code>Node Interaction</code> and then select <code>Create node</code>. Each node, like a pipeline, is assigned a unique identifier (<code>node_id</code>) to ensure its uniqueness within a pipeline. A specific <code>node_id</code> can appear only once in a given pipeline, but the same node can be referenced in multiple pipelines, allowing for shared use and consistency. See Develop a Node for more information.</p> <p>A node is represented as a folder containing three subfolders:</p> <ul> <li><code>data</code>: stores the output generated by the node.</li> <li><code>code</code>: contains your scripts and code files.</li> <li><code>reports</code>: used for saving plots or reports produced during your analysis.</li> </ul> <p>This structure helps organize your workflow by keeping data, code, and results neatly separated within each node.</p> <p>If you want to change the tag and add notes to your node, you can select the node and open the <code>Node interaction</code> options and select <code>Open selected node panel</code>. </p>"},{"location":"user_guide/pipeline_interaction/#navigate-to-your-node-folder-with-your-favourite-ide","title":"Navigate to your node folder with your favourite IDE","text":"<p>To get the folder path containing the node, right click on the the node and <code>Copy Folder Path</code>. This will copy in the clipboard the absolute path to the folder containing the node information. </p> <p>You can now use your favourite IDE to modify the code in your node.</p>"},{"location":"user_guide/pipeline_interaction/#connect-nodes","title":"Connect nodes","text":"<p>Each node exposes two connection points: an <code>input port</code> and an <code>output port</code>. These ports enable inter-node connections within your pipeline. When you create a new node and connect the <code>output port</code> of your first node to the <code>input port</code> of the second node, FusionPipe automatically establishes a parent-child relationship in its database. This relationship allows child nodes to access the output data generated by their parent nodes, creating a seamless data flow through your pipeline.</p> <p>To automatically organize the nodes in your pipeline workspace, access the <code>Layout</code> menu and click <code>Auto reshape</code>. This feature arranges all nodes in an optimal layout for better visualization.</p>"},{"location":"user_guide/pipeline_interaction/#delete-nodes-and-connections","title":"Delete nodes and connections","text":"<p>To delete a node, click on it to ensure it is selected (the edge color will change to indicate selection). Then go to <code>Node interaction</code> (dropdown menu) and choose <code>Delete selected nodes</code> option.</p> <p>You can also delete multiple nodes at once by holding <code>Shift</code> and dragging with the left mouse button to select them, then using the same delete option.</p> <p>To clear only a node\u2019s outputs while preserving its code, select the node and choose <code>Delete output selected nodes</code>. This action will empty the node\u2019s <code>data</code> folder, but the node itself and its <code>code folder</code> will remain unchanged.</p> <p>It is possible to delete connections between node: just select an edge which is connecting an <code>output port</code> and an <code>input port</code> and from <code>Node interaction</code> click on <code>Delete selected edge</code>. </p>"},{"location":"user_guide/pipeline_interaction/#node-status","title":"Node status","text":"<p>A node can be in one of the following five states:</p> <p>Ready: The node is ready to run; its output folder is empty.</p> <p>Completed: The node has run successfully and produced output.</p> <p>Running: The node is currently being executed.</p> <p>Failed: The node execution failed. Check the <code>log.txt</code> file in the node folder for debugging information.</p> <p>Stale-data: The node\u2019s output may be inconsistent with its code or inputs. This indicates the pipeline should be rerun to update results. See Stale-data section below for more information.</p>"},{"location":"user_guide/pipeline_interaction/#actions","title":"Actions","text":"<p>Select a node and run: to run a node, select it and open the <code>Actions</code> panel. Then click on <code>Run selected node</code>.  This execution will work only if the node has no parents and its <code>data folder</code> is empty, or all the parents of the node are in the status completed. Now, you can go on <code>Layout</code> and select <code>Refresh pipeline</code> to see your node entering the running state and finally the completed state.</p> <p>Run full pipeline: from the <code>Actions</code> panel, select <code>Run full pipeline</code> to execute all nodes in your pipeline sequentially. FusionPipe will automatically determine the correct order based on node dependencies, ensuring that each node runs only after its parent nodes have completed. This feature streamlines the workflow, allowing you to process the entire pipeline with a single action.</p> <p>Run pipelione up to a node: to execute your pipeline sequentially up to a node, select a node and from the <code>Actions</code> panel, select <code>Run pipeline up to selected node</code>.</p> <p>Kill running node:  to stop the process of a node that is running, select that node, open <code>Actions</code> panel and click on <code>Kill run selected node</code>. That node will switch to failed sate. </p>"},{"location":"user_guide/pipeline_interaction/#node-in-status-stale-data","title":"Node in status stale-data","text":"<p>The status <code>stale-data</code> is a special status which allows the user to flag the the output of a node are no longer consistent with the code of the node or the its inputs, and the pipeline needs to be rerun</p> <p>A node can transition from completed or ready to stale-data state. This can happen in the following situations:</p> <ul> <li>The user has changed the code in a node. Then it can flag the node to be in stale-data state. This way the pipeline needs to re-run the node and all its children.</li> <li>A node was in completed state, but then the user changed the inputs of the node.</li> <li>If there are multiple connected nodes, which are in completed state and you delete the output of one of these nodes. In this case, all the children nodes will switch to stale-data state.</li> <li>If you have multiple nodes, connected, which are in completed state and you delete the edge (connection) of a node. All the children of this edge enters the state stale-data state.</li> <li>When you duplicate a node, with data, the duplicated node by default will be in the stale-data state.</li> </ul> <p>Tip</p> <p>Sometimes, FusionPipe may automatically set a node to the <code>stale-data</code> status after one of the previous condition, when the node was previously in the status <code>completed</code>. However, there may be cases where you are confident that the node\u2019s output remains valid and does not require rerunning. In such situations, you can manually set the node back to <code>completed</code> from the <code>Node Interaction</code> panel. This is especially helpful for nodes with outputs that took significant time to generate.</p>"},{"location":"user_guide/pipeline_interaction_advanced/","title":"Advanced Interaction","text":""},{"location":"user_guide/pipeline_interaction_advanced/#advanced-user-interaction","title":"Advanced user interaction","text":""},{"location":"user_guide/pipeline_interaction_advanced/#duplicate-or-reference-nodes","title":"Duplicate or reference nodes","text":"<p>You can select one or more nodes with <code>shift + right click</code>. Once you have selected the desired nodes, you can duplicate them using several available options. Open the <code>Node interaction</code> panel to see three different options:</p> <p><code>Duplicate selected nodes in this pipeline</code>: this command is used to duplicate the nodes in the same pipeline. You can choose to copy the node with or without data (<code>Duplicate with data</code>, <code>Duplicate without data</code>). The new node will have a new unique identifier, but the same tag as the original node.</p> <p><code>Duplicate selected nodes into another pipeline</code>: duplicate the node into a different pipeline (select the pipeline from the option <code>Select a pipeline...</code>) with or without data (<code>Duplicate with data</code>, <code>Duplicate without data</code>). The new node will have a new unique identifier, but the same tag as the original node.</p> <p><code>Reference selected nodes into another pipeline</code>: the node is not duplicated. Select the pipeline with <code>Select a pipeline...</code>, then click on <code>Reference nodes</code>. The same <code>node_id</code> is referenced in a different pipeline and the <code>node</code> is set in the status locked. This means that if you change the code or the data in this node, it will be changed in all the pipelines that reference this node. A lock symbol () will appear in that node and all of its references in the other pipelines. This is especially convenient when you want to use the output of the node in a different pipeline, without duplicating the data.</p> <p>Info</p> <p>We remind the user that a <code>node_id</code> is unique within a given pipeline. But the same <code>node_id</code> can appear in multiple pipeline after a reference operation.</p>"},{"location":"user_guide/pipeline_interaction_advanced/#locked-nodes","title":"Locked nodes","text":"<p>When a node is in \"locked\" status (), it indicates that this node is referenced by multiple pipelines. Any changes made to the code or data within a locked node will automatically propagate to all pipelines that reference it.</p> <p>Warning</p> <p>In general, you should NOT modify the code or outputs of a <code>locked</code> node, as this may unintentionally affect other pipelines or colleagues who also reference this node. If you need to make changes to a <code>locked</code> node within your pipeline, it is recommended to first duplicate the node and then modify the duplicated version.</p>"}]}